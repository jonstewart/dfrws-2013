\documentclass[5p,final,number,sort&compress]{elsarticle}

\usepackage{lmodern}
\usepackage{textcomp}

\usepackage{graphicx}

\usepackage{ucs}
\usepackage[utf8x]{inputenc}
\usepackage[greek,english]{babel}
\usepackage[LUC,T1]{fontenc}

\makeatletter
\ProvideTextCommandDefault{\textunicodechar}[1]{\uni@char{#1}}
\makeatother

\newcommand{\re}[1]{\texttt{#1}}
\newcommand*{\whack}{\textbackslash}

\usepackage{amsmath}
\usepackage{microtype}
\usepackage{natbib}

\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{decorations}
\usetikzlibrary{decorations.pathreplacing}
\usetikzlibrary{matrix}
\usetikzlibrary{positioning}
\usetikzlibrary{shapes.arrows}

\pgfdeclarelayer{background}
\pgfdeclarelayer{foreground}
\pgfsetlayers{background,main,foreground}

\usepackage{xcolor}

\newcommand{\ju}[1]{\textcolor{red}{\footnote{\textcolor{red}{ju: #1}}}}

\newcommand{\todo}[1]{\textcolor{red}{#1}}

\begin{document}
\begin{frontmatter}

\title{Search in a Multi-Encoding World, Or:\\ How I Learned to Stop Worrying and Love Unicode Technical Standard \#18}

\author{Jon Stewart}
\ead{jon@lightboxtechnologies.com}

\author{Joel Uckelman}
\ead{joel@lightboxtechnologies.com}

\address{Lightbox Technologies, Inc. \\ Arlington, VA}

\begin{abstract}
We needed to search a disk image for a pattern, but didn't know how the hits would be encoded. We thought very hard about how to do this. We found an efficient solution.
\end{abstract}

\end{frontmatter}

\section{Introduction}
% in which we say what we will say

\section{Character Encoding Basics}
% in which character encodings are explained

\section{Multipattern Search is Multiencoding Search}
\label{sec:multi}
% in which we sketch a general solution to the problem

A \emph{coded character set} is list of pairs, each consisting of a character and the unique integer represeting it, known as a \emph{code point}.  An \emph{encoding} is a method for mapping sequences of code points to sequences of bytes. Examples: Unicode is a coded character set consisting of 1,114,112 code points, intended to be sufficient for representing all text produced by humans. UTF-8 and UTF-16 are encodings capable of representing all Unicode code points as bytes. ASCII, commonly used for simple text files (especially by English speakers), is both a coded character set and an encoding---the 128 code points in ASCII, numbered 0--127, are directly encoded as bytes 0--127. Numerous encodings specific to one or more natural languages were developed---such as Shift\_JIS, EUC-KR, KOI8-R, and ISO 8859-1---which are slowly being supplanted by UTF-8 and UTF-16.

The multiplicity of encodings means that one piece of text can be represented as bytes in numerous ways. E.g., the text ``IRLIBSYR'' can be encoded as in Figure~\ref{fig:enc}.

\begin{figure*}[th]
\centering
\begin{tikzpicture}
  \small
  \matrix (m) [matrix of nodes,nodes in empty cells,nodes={inner sep=0pt,outer sep=0pt,minimum width=5mm},column sep={5mm,between origins},row sep=4ex]
  {
    49 & 52 & 4C & 49 & 42 & 53 & 59 & 52 &    &    &    &    &    &    &    & \\
    49 & 00 & 52 & 00 & 4C & 00 & 49 & 00 & 42 & 00 & 53 & 00 & 59 & 00 & 52 & 00 \\
    C9 & D9 & D3 & C9 & C2 & E2 & E8 & D9 \\
    I  & R  & L  & I  & B  & S  & Y  & R \\
  };

  \draw ([xshift=-2.5mm,yshift=0.5em]m-1-1.north) rectangle ([xshift=2.5mm,yshift=-0.5em]m-1-8.south);

  \foreach \x in {1,...,7}
    \draw ([xshift=2.5mm,yshift=0.5em]m-1-\x.north) -- ([xshift=2.5mm,yshift=-0.5em]m-1-\x.south);

  \draw ([xshift=-2.5mm,yshift=0.5em]m-2-1.north) rectangle ([xshift=2.5mm,yshift=-0.5em]m-2-16.south);

  \foreach \x in {1,...,16}
    \draw ([xshift=2.5mm,yshift=0.5em]m-2-\x.north) -- ([xshift=2.5mm,yshift=-0.5em]m-2-\x.south);

  \draw ([xshift=-2.5mm,yshift=0.5em]m-3-1.north) rectangle ([xshift=2.5mm,yshift=-0.5em]m-3-8.south);

  \foreach \x in {1,...,7}
    \draw ([xshift=2.5mm,yshift=0.5em]m-3-\x.north) -- ([xshift=2.5mm,yshift=-0.5em]m-3-\x.south);

  \node [left=5mm of m-1-1.west] {ASCII and UTF-8};
  \node [left=5mm of m-2-1.west] {UTF16-LE};
  \node [left=5mm of m-3-1.west] {EBCDIC 37};

  \foreach \x in {1,...,16}
    \pgfmathparse{\x-1}
    \pgfmathtruncatemacro{\n}{\pgfmathresult}
    \draw (m-1-1.north -| m-1-\x) node [above=2mm] {\tiny \n};

  \begin{pgfonlayer}{background}
    \foreach \x/\color in {1/red, 2/blue, 3/green, 4/yellow, 5/orange, 6/teal, 7/magenta, 8/cyan}
      \pgfmathparse{2*(\x-1)+1}
      \pgfmathtruncatemacro{\xx}{\pgfmathresult}
      \pgfmathparse{2*\x}
      \pgfmathtruncatemacro{\xxx}{\pgfmathresult}
      \fill [color=\color!30]
        ([xshift=-2.5mm,yshift=0.5em]m-1-\x.north) --
        ([xshift=-2.5mm,yshift=-0.5em]m-1-\x.south) --
        ([xshift=-2.5mm,yshift=0.5em]m-2-\xx.north) --
        ([xshift=-2.5mm,yshift=-0.5em]m-2-\xx.south) --
        ([xshift=-2.5mm,yshift=0.5em]m-3-\x.north) --
        ([xshift=-2.5mm,yshift=-0.5em]m-3-\x.south) --
        ([xshift=2.5mm,yshift=-0.5em]m-3-\x.south) -- 
        ([xshift=2.5mm,yshift=0.5em]m-3-\x.north) --
        ([xshift=2.5mm,yshift=-0.5em]m-2-\xxx.south) --
        ([xshift=2.5mm,yshift=0.5em]m-2-\xxx.north) --
        ([xshift=2.5mm,yshift=-0.5em]m-1-\x.south) --
        ([xshift=2.5mm,yshift=0.5em]m-1-\x.north) --
        cycle;

    \foreach \x/\color in {1/red, 2/blue, 3/green, 4/yellow, 5/orange, 6/teal, 7/magenta, 8/cyan}
      \draw [line width=5mm,color=\color!30,cap=round,join=round]
        (m-1-\x.north) -- (m-1-\x) (m-3-\x) -- (m-4-\x.south);
  \end{pgfonlayer}
\end{tikzpicture}
\caption{Several encodings of the text ``IRLIBSYR''\label{fig:enc}}
\end{figure*}

The UTF-16LE encoding, while containing values similar to the UTF-8 encoding, is twice the length, while the EBCDIC 37 encoding bears no resemblance to the other two. Searching a block of bytes for ``IRLIBSYR'' can mean searching for ``IRLIBSYR'' once for each possible encoding. Hundreds of encodings exist, and while the vast majority of them are obsolete and unlikely to be encountered in data from a contemporary computer, there are still a few dozen which are in common usage. Therefore, if we are trying to establish the existence of a particular string within data for which we don't know the encoding beforehand, the search task can be rather large. Should we need to do this for hundreds, or even thousands of strings (or, in the general case, regular expressions), then it will require a prohibitive amount of time to carry out the search serially, reading the data once for each pattern-encoding pair.

Fortunately, multipattern search rescues us in this situation: Searching in parallel for one string rendered in multiple encodings is structurally the same problem as searching in parallel for multiple strings each rendered in one encoding. Hence, a search for the pattern \texttt{IRLIBSYR.*?SACPOP} in UTF-8, UTF-16LE, UTF-16BE, UTF-32LE, and UTF-32BE does not necessitate five passes over the data (once for each encoding), but merely adds five patterns to the pattern set from which we build our search automaton.

%\PreloadUnicodePage{92}
%\PreloadUnicodePage{182}

%The literals in our regexes are Unicode characters, \emph{not} bytes in a particular Unicode encoding (such as UTF-8). This distinction isn't present when working with traditional ASCII regular expressions, where the characters and the bytes are identical. Distinguishing between characters and bytes in Unicode regular expressions makes the regexes \emph{encoding-independent}, which is a useful property should one wish to search for the same character string in several different encodings.\ju{Example here?} Furthermore, having literals be Unicode characters can improve regex readability over specifying them by code point: While
%\texttt{\textchinesetrad{屁股}} and \texttt{\textchinesetrad{地洞}}
%are easily confusable for those unfamiliar with Chinese, and thus specifying the characters by code point may be called for, of the two patterns matching the Greek name for the city of Athens, even non-Grecophones are likely to prefer \re{\textgreek{Αθήνα}} to \re{\whack x\{0391\}\whack x\{03B8\}\whack x\{03AE\}\whack x\{03BD\}\whack x\{03B1\}}.


\section{Fun With Unicode}
% in which fun is not actually had

Unicode Technical Standard \#18 (hereafter, UTS~\#18) provides guidelines for supporting Unicode characters in regular expressions \citep{uts18}.

% FIXME: missing RL1.2a
% FIXME: also, what happened to RL2.1?

UTS~\#18 defines three levels of support for Unicode in regexes. Level~1, Basic Unicode Support, requires support for specifying Unicode characters by their code points (RL1.1), selectors for Unicode properties (RL1.2), character class union, subtraction, and intersection (RL1.3), simple word boundary matching (RL1.4), simple loose matching (RL1.5), (RL1.6) line boundary matching, and expression of the full range of Unicode characters (RL1.7). Level~2, Extended Unicode Support, requires support for matching cannonical equivalents (RL2.1), extended grapheme clusters (RL2.2), default word boundaries (RL2.3) and name properties (RL2.5), all Unicode properties (RL2.7), as well as support for full default case folding and (RL2.4) and wildcards in property names (RL2.6). Level~3, Tailored Support, requires locale-specific punctuation properties (RL3.1), grapheme clusters (RL3.2), and word boundaries (RL3.3), as well as context (RL3.6) and incremental (RL3.7) matching, the generation of posssible match sets (RL3.9), and match subroutines (RL3.11). We now proceed to explain each of the reqiurements.

\subsection{Level~1: Basic Unicode Support}

%\PreloadUnicodePage{253}

RL1.1 requires Level~1-conformant regex languages to provide a way of specifying code points in hexadecimal. Following PCRE, we meet this requirement with the metacharacter \re{\whack x}, which takes a hexadecimal code point as its argument. E.g., \textsc{latin small letter a} (a) \textsc{arabic ligature sallallahou alayhe wasallam} ( 
%ﷺ
), which is virtually illegible on-screen at 10pt, can be specified as \re{\whack x\{61\}} and \re{\whack x\{FDFA\}}, respectively.

\PreloadUnicodePage{0}

RL1.2 requires Level~1-conformant regex languages to support the character classes defined by the Unicode properties General Category, Script, Alphabetic, Uppercase, Lowercase, White Space, Noncharacter Code Point, Default Ignorable Code Point, Any, ASCII, and Assigned. Unicode properties are fundamentally sets of characters. The General Category and Script properties are multivalued, while the other properties are binary. For example, any given character is either Uppercase or not, while the Script for a letter might be Arabic, Latin, Greek, Cyrillic, or any of 100 other scripts defined in Unicode~6.2.\footnote{For a complete list, see the Unicode~6.2 data file \texttt{PropertyValueAliases.txt} \citep{pva62}.} Following PCRE, we meet this requirement with the metacharacter \re{\whack p}, which takes a Unicode property name (with possible value) as its argument. E.g., \re{\whack p\{Script=Arabic\}} matches any character in Arabic script; \re{\whack p\{General Category=Currency Symbol\}} matches currency symbols such as \$, ¢, \texteuro, and ¥; and \re{\whack p\{White Space\}} matches spaces, tabs, newlines, and carriage returns, as well as more exotic whitespace such as \textsc{mongolian vowel separator}.  \re{\whack p\{Any\}} is the class of all Unicode code points, while \re{\whack p\{Assigned\}} is the class of all code points with a character assigned to them. UTS~\#18 recommends (but does not require) leniency with regard to spaces, case, hyphens, and underscores---which we respect. Hence, \re{\whack p\{Whitespace\}}, \re{\whack p\{wHiTeSpAcE\}}, and \re{\whack p\{\_\_\_white\;\;\;\;space\_\_\_\}} are equivalent. The Unicode Standard defines numerous short forms of properties (such as ``Khmer'' for ``Script=Khmer''), which we also support.

RL1.3 requires Level~1-conformant regex languages to support union, intersection, and set difference on character classes. E.g., one might wish to search for anything Cyrillic or Greek (the union of Cyrillic and Greek scripts), or lowercase Latin letters (the intersection of the Latin letters and the lowercase letters, or, equivalently, the Latin letters minus the uppercase letters). We support only union at present, by permitting \re{\whack p} and other named character classes to appear as members of the bracketed character class construct. \re{[\whack p\{Cyrillic\}\whack p\{Greek\}]} will match any character in either the Cyrillic or Greek scripts. We intend to implement intersection and set difference as part of future work. 

RL1.4 requires Level~1-conformant regex languages to treat all characters having property Alphabetic or Decimal Number as well as the \textsc{zero width non-joiner} and \textsc{zero width joiner} characters as being word characters for the purpose of matching word boundaries, and additionally that combining marks (such as \textsc{combining ring above}) are not separated from their base characters by word boundaries. (A word boundary customarily occurs between any two consecutive characters where one is a word character and the other not, and also at the beginnings and ends of strings. Word boundaries, due to occurring between characters, have zero width.) We do not presently support word boundaries, but doing so is trivial once we add support for positive look-ahead and -behind assertions.

RL1.5 requires Level~1-conformant regex languages to support default case-insensitive matching for all Unicode characters if they support case-insensitive matching at all, and additionally to indicate which properties will be closed under simple default case folding when matching case-insensitively. In order to unpack this requirement, we need to understand default case-insensitive matching, closure, and simple default case folding. Default case-insensitive matching, as defined in \citep[\S 3.1.3]{ustd62}, is too complex to explain here. It suffices to say that the standard defines the function $\operatorname{toCasefold}$ for every string, and two strings $S,T$ are default case-insensitively equivalent iff $\operatorname{toCasefold}(S) = \operatorname{toCasefold}(T)$. Thinking of case-folding as simple lowercasing is fairly close, but misses a few corner cases that the standard handles. In order to fulfil the requirement of RL1.5, a pattern interpreted case-insensitively must match all strings which are case-insensitively equivalent to the strings that same pattern would match case-sensitively. This requirement touches on a feature of Unicode which surprises novices, namely that lowercasing and uppercasing are not inverses. According to the Unicode Standard, both \textsc{latin capital letter k} (K) and the \textsc{kelvin sign} (K) lowercase to \textsc{latin small letter k}, but \textsc{latin small letter k} (k) uppercases to \textsc{latin capital letter k} (K) only.\footnote{Note that in many fonts, the Kelvin sign is indistinguishable from a capital K.} Both \textsc{latin small letter s} (s) and \textsc{latin small letter long s} (
%ſ
) uppercase to \textsc{latin capital letter s} (S), but \textsc{latin capital letter s} lowercases to \textsc{latin small letter s} only.\footnote{You might have seen a long s, also known as a medial s, in books from the 17th century (or fascimilies thereof). E.g., the title page of the 1668 edition of Milton's \emph{Paradise Lost} spells the title as \emph{Paradi
%ſ
e Lo
%ſ
t}.}
A consequence of this is that the character class $\re{[A-Z]}$ matches not 52, but 54 different letters when interpreted case-insensitively in an encoding supporting full Unicode. Now for property closure: Recall that properties can be thought of as sets of characters. Let $C_0$ be a set, $o$ a function from sets to sets, and $C_n = o(C_{n-1}) \cup C_{n-1}$ for $n > 0$. The set which is the closure of $C_0$ under $o$ is the $C_i$ such that $C_i = C_{i+1}$. (And, in fact, if $C_i$ is closed under $o$, then $C_i = C_j$ for all $j \ge i$.) That is, the closure of $C_0$ under $o$ is the set from which no new elements are generated when $o$ is iteratively applied to it. E.g., the set $\{\mathrm{A}\}$ is not closed under simple default case folding because A case-folds to a, and $\mathrm{a} \notin \{\mathrm{A}\}$. The set $\{\mathrm{A}, \mathrm{a}\}$ is closed under simple default case-folding, since A case-folds to a and a case-folds to A. Now it should be clear what it would mean for a Unicode property to be closed (or not) under case-folding when matching case-insensitively. The property General Category=Letter, by virtue of containing all letters, whether upper-, lower- or uncased, cannot fail to be closed under simple default case-folding when matching case-insensitively in any correct implementation, but whether an implementation similarly closes the property General Category=Uppercase Letter is up to the implementers. It would be strange---though permissible---if \re{\whack p\{General Category=Uppercase Letter\}} in a case-insensitive pattern did not match lowercase letters. We chose to close all properties under simple default case-folding when matching case-insensitively, in accordance with the principle of least surprise.

RL1.6 requires Level~1-conformant regex languages to recognize all of LF (U+0A), CR (U+0D), CR LF (U+0A U+0D), NEL (U+85), \textsc{line separator} (U+2028), and \textsc{paragraph separator} (U+2029) as ending logical lines. (LF is the traditional end-of-line character on UNIX, CR LF on Windows, CR on MacOS prior to OS X. Customarily, ASCII regex implementations would recognize some combination of these as line endings, perhaps depending on the platform where they were running.) The start- and end-of-line assertions \re{\^} and \re{\$} are not terribly useful when searching data, such as disk images, which is not line-oriented. Hence, we support RL1.6 vacuously by not supporting line-break assertions at all.

RL1.7 requires Level~1-conformant regex languages to handle all code points, as well as treating UTF-16 surrogate pairs as single code points when matching. In the early days of Unicode, some encoders and decoders balked at valid characters above U+7FF (the last two-byte character in UTF-8) or U+FFFF (the last two-byte character in UTF-16); some UTF-16 decoders also incorrectly decoded surrogate pairs as two invalid code points rather than single valid ones. These two requirement are aimed at heading off such shoddy implementations. We support all code points and correctly interpret UTF-16 surrogate pairs. 

\subsection{Level~2: Extended Unicode Support}

RL2.2 requires Level~2-conformant regex languages to provide a way of matching an any extended grapheme cluster, a literal cluster, and extended grapheme cluster boundaries. An \emph{extended grapheme cluster} is the glyph that from a user's point of view consists of a single character.\footnote{This is a gloss. For the definition, see \citep[\S 3]{uax29}.} E.g., the sequence of the two Unicode characters \textsc{latin small letter o} and \textsc{combining double acute accent} form the extended grapheme cluster \H{o} used in Hungarian (which can also be produced by the single Unicode character \textsc{latin small letter o with double acute}). The metacharacter for matching any extended grapheme clusters is analogous to the dot metecharacter for matching any single code point. PCRE uses \re{\whack X} for this purpose. E.g., on the sequence $\langle \text{o U+030B} \rangle$, the regex \re{.} matches the o, while \re{\whack X} matches the o and its combining accent as a unit. The requirement for matching literal clusters is intended to facilitate including them in character classes, e.g., as in \re{[\H{o}\whack q\{o\whack x\{030B\}]\}} which follows the UTS~\#18 sample syntax and is intended to match either representation of~\H{o}. Support for literal clusters is vexing, as it means that a character class can no longer be expected always to match a single character---in the example, one member of the class is two Unicode characters long, instead of one. We do not yet support any of RL2.2.

RL2.3 requires Level~2-conformant regex languages to provide assertions for matching Unicode default word boundries, as defined in \citep[\S 4]{uax29}. This requirement is far more demanding than simple word boundaries as defined in RL1.4, and we do not presently support it.

RL2.4 requires Level~2-conformant regex languages to provide full default Unicode case folding if they provide case conversion at all. The salient difference between this requirement and that of RL1.3 can be seen for characters like U+DF, \textsc{latin small letter sharp s} (ß). In German, ß capitalizes to SS. This causes common words (e.g., straße) to change length when converted to uppercase (STRASSE). Similar things apply to \textsc{latin capital ligature ij} (
%Ĳ
), \textsc{latin small ligature ij} (
%ĳ
), and the sequences IJ and ij, all of which are considered a single letter in Dutch; the digraphs \textsc{latin small letter dz} (
%ǳ
), \textsc{latin capital letter dz} (
%Ǳ
), and \textsc{latin capital letter d with small letter z} (
%ǲ
), used in some Slavic languages; as well as the various ligatures 
%ﬀ
,
%ﬁ
, 
%ﬂ
, etc.\ used in quality printing. Full default case folding should make the pattern \re{straße} case-insensitively match all of the following: strasse, straße, STRASSE. While this feature is undoubtedly useful, especially for investigators unaware of the subtleties of various languages, the primary difficulty with implementing RL2.4 is the same as with RL2.2, namely that character classes can sometimes contain character sequences in addition to single characters, and as such, we do not yet support it.

RL2.5 requires Level~2-conformant regex languages to support referring to characters by name. Many Unicode characters, such as \textsc{apl functional symbol tilde diaresis} (
%⍨
) and \textsc{pile of poo} ( ), are not easy to type and have descriptive names more memorable than their hexadecimal code points; therefore, being able to specify them by name might on occasion be useful. Following PCRE, we provide the \re{\whack N} metacharacter, which takes the name of a character as its argument and matches the named character, as well as supporting the Name property via \re{\whack p}. E.g., \textsc{thai character tho phuthao} (
%ฒ
) is matched both by \re{\whack N\{thai character tho phuthao\}} and \re{\whack p\{name=thai character tho phuthao\}}.

RL2.6 requires Level~2-conformant regex languages to support wildcards in Unicode property values. The suggested syntax is more expansive, permitting arbitrary regular expressions to appear as property value matchers. For example, \re{\whack p\{name=/l.*tel/\}} would produce a chacter class containing \textsc{byzantine musical symbol apostrofoi telous ichimatos} ( ), \textsc{black telephone} (), and \textsc{love hotel} ( ), among others. We do not yet support this, as we do not yet have a compelling use case for it.

RL2.7 requires Level~2-conformant regex languages to support all (non-provisional, non-contributory, non-obsolete, non-deprecated) Unicode properties. This amounts to several dozen propeties beyond those required by RL1.2, such as Hex Digit, Changes When Lowercased, and Ideographic. We support these properties.

\subsection{Level~3: Tailored Support}

RL3.1 requires Level~3-conformant regex languages to have locale-specific punctuation properties. For example, \textsc{left-pointing double angle quotation mark} («) and \textsc{right-pointing double angle quotation mark} (») might be considered Punctuation in a French locale, but not in an English one.

RL3.2 requires Level~3-conformant regex languages to have locale-specific collation order for collation grapheme clusters. The most striking effect of a locale's collation order on regexes involves ranges in character classes. The standard collation order used for ranges in character classes is ascending code point order. In the standard, locale-agnostic collation the character class \re{[O-P]} matches O and P only, which is correct for many languages---but not for German, which sorts \"O between O and P. (This problem is not solvable simply by declaring that \"O always sorts between O and P, as then the standard collation would be wrong for Swedish, where \"O sorts as the last letter of the alphabet.)  Hence, a regex language conforming to this requirement should provide a collation order which respects the locale selected by the user.

RL3.3 requires Level~3-conformant regex languages to have locale-specific word boundaries. Default word boundaries (RL2.3) are insufficient for splitting words in some languages. Thai, for example, has no interword whitespace, so detecting word boundaries is not a simple matter of looking for alternations between word and non-word characters.

We chose not to support RL3.1--3 because using them correctly requires language-specific expertise that forensics investigators are unlikely to possess\footnote{To appreciate the problem, consider for how many laguages you are familiar with their collation order.}; given that, implementing these for more than a handful of locales is a gargantuan task with little benefit for forensics.

RL3.6 requries Level~3-conformant regex languages to support unmatchable leading and trailing context. For a given input text, a target range is specified within which matches may occur, which range sits within a context range where matches may not occur, though the context may be referred to by zero-width look-around assertions. Look-around assertions clearly are useful in forensics, and we intend to implement them; but we see no obvious use in forensics for the remainder of this requirement.

RL3.7 requires Level~3-conformant regex languages to support incremental matching. Normally, a pattern which has not yet matched by the end of input fails to match. E.g., the pattern \re{abc} has no match on the text \texttt{ab}---but, were more text appended, \texttt{cde}, for example, then \re{abc} could still match. Incremental matching is highly desirable for forensics, in particular when searching data which does not fit into RAM, such as disk images. Typically, any large block of data is searched by reading a chunk into a buffer in RAM, searching the buffer, refilling the buffer with the next chunk of data, etc. until all data is exhausted. If partial matches are not carried over from one fill of the buffer to the next, then either matches spanning chunks will be missed, or some horiffically complicated algorithm will required to find cross-chunk matches. By keeping partial matches alive across buffer fills, problems with matches spanning two or more buffers vanish. We support incremental matching by permitting matches to span an unlimited\footnote{Where ``unlimited'' $< 2^{64}$. We don't expect anyone to need offsets larger than 64 bits, at least not in 2013.} number of buffers.

RL3.9 requires Level~3-conformant regex languages to support the generation of possible match sets for any given pattern. Novice regex users have always been plagued by unexpected matches and nonmatches. With the expansion of regexes to Unicode, the possibility that a given regex could match unexpectedly increases---hence, there is great practical utility in being able to see sample matches for any given pattern, especially if that means correcting a faulty pattern before running an hours- or days-long search. We support generation of possible matches for a pattern by randomly walking its search automaton, reading off a match for each complete branch walked.

RL3.11 requires Level~3-conformant regex languages to permit registration of arbitrary submatching functions. We have no intention of supporting this, as the injection of arbitrary code into our match engine is incompatible with providing performance guarantees.

\subsection{UTS~\#18 Conformance}

\begin{table}
\small
\centering
\setlength{\tabcolsep}{3pt}
\begin{tabular}{l|c|c|c|c|c|}
 \multicolumn{1}{c}{}
 & \multicolumn{1}{c}{\rule{1em}{0pt}\makebox[0cm][r]{\rotatebox[origin=rB]{-45}{lightgrep}}}
 & \multicolumn{1}{c}{\rule{1em}{0pt}\makebox[0cm][r]{\rotatebox[origin=rB]{-45}{ICU 50}}}
 & \multicolumn{1}{c}{\rule{1em}{0pt}\makebox[0cm][r]{\rotatebox[origin=rB]{-45}{Perl 5.6}}}
 & \multicolumn{1}{c}{\rule{1em}{0pt}\makebox[0cm][r]{\rotatebox[origin=rB]{-45}{Java 7}}}
 & \multicolumn{1}{c}{\rule{1em}{0pt}\makebox[0cm][r]{\rotatebox[origin=rB]{-45}{Python regex}}} \\
\hline
Level 1: Basic Unicode Support     & $\circ$   & $\bullet$ & $\circ$   & $\bullet$ & $\bullet$ \\
\hline
RL1.1 Hex Notation                 & $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ \\
RL1.2 Properties                   & $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ \\
RL1.3 Subtraction and Intersection & $\circ$   & $\bullet$ & $\circ$   & $\bullet$ & $\bullet$ \\
RL1.4 Simple Word Boundaries       &           & $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ \\
RL1.5 Simple Loose Matches         & $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ \\
RL1.6 Line Boundaries              &           & $\bullet$ & $\circ$   & $\bullet$ & $\bullet$ \\
RL1.7 Supplementary Code Points    & $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ \\
\hline
Level 2: Extended Unicode Support  & $\circ$   & $\circ$   & $\circ$   & $\circ$   & $\circ$   \\
\hline
RL2.1 Canonical Equivalents        &           &           &           & $\bullet$ &           \\
RL2.2 Extended Grapheme Clusters   &           & $\circ$   & $\circ$   &           & $\bullet$ \\
RL2.3 Default Word Boundaries      &           & $\bullet$ &           &           &           \\
RL2.4 Default Case Conversion      &           &           &           &           &           \\
RL2.5 Name Properties              & $\bullet$ & $\bullet$ & $\bullet$ &           & $\bullet$ \\
RL2.6 Wildcards in Property Values &           &           &           &           &           \\
RL2.7 Full Properties              & $\bullet$ &           & $\bullet$ &           & $\bullet$ \\
\hline
Level 3: Tailored Support          & $\circ$   &           & $\circ$   &           &           \\
\hline
RL3.1 Tailored Punctuation         &           &           &           &           &           \\
RL3.2 Tailored Grapheme Clusters   &           &           &           &           &           \\
RL3.6 Context Matching             &           &           & $\circ$   &           &           \\
RL3.7 Incremental Matches          & $\bullet$ &           &           &           &           \\
RL3.9 Possible Match Sets          & $\bullet$ &           &           &           &           \\
RL3.11 Submatchers                 &           &           &           &           &           \\
\hline
\end{tabular}

\medskip
$\bullet = $ full support, $\circ = $ partial support
\caption{UTS~\#18 Support\label{tab:uts18-support}}
\end{table}

Table~\ref{tab:uts18-support} summarizes UTS~\#18 conformance for lightgrep, as well as for ICU~50 \citep{icuregex}, Perl~5.6 \citep{perlunicode}, Java~7 \citep{jdk7pattern}, and the Python regex library \citep{pythonregex}.

\section{Encoding Chains}
\label{sec:chains}

\begin{figure*}[ht]
\centering
\tikzstyle{func} = [draw,rounded corners,minimum width=6mm]
\begin{tikzpicture}[every to/.style={thick,->}]

  \matrix (m) [matrix of math nodes,nodes in empty cells,column sep={10mm,between origins},nodes={outer sep=0pt,minimum height=6mm,anchor=center}] {
    & |[func]| c_0 & |[func]| c_1 & \cdots & |[func]| c_n & |[func]| e & |[func]| b_0 & |[func]| b_1 & \cdots & |[func]| b_n & \\
  };

  \foreach \x in {1,...,10}
    \pgfmathparse{\x+1}
    \pgfmathtruncatemacro{\xx}{\pgfmathresult}
    \path[every edge/.style={draw,semithick,->}] (m-1-\x) edge (m-1-\xx);

  \draw [semithick,decorate,decoration={brace,amplitude=5pt,raise=3pt}] (m-1-5.south east) -- (m-1-2.south west) node [midway,below=8pt,text width=4cm,text centered] {\small code point-code point\\ transformations};

  \draw [semithick,decorate,decoration={brace,amplitude=5pt,raise=3pt}] (m-1-10.south east) -- (m-1-7.south west) node [midway,below=8pt,text width=3cm,text centered] {\small byte-byte\\ transformations};

  \draw [semithick] ([yshift=-3pt]m-1-6.south) -- +(0,-1.25cm) node [below] {\small code point-byte transformation};

\end{tikzpicture}
\caption{An abstract transformation chain\label{fig:abs-chain}}
\end{figure*}


\begin{figure*}[ht]
\centering
\small
\begin{tikzpicture}
  \node (text) {IRLIBSYR};

  \matrix (utf8) [matrix of nodes,nodes in empty cells,nodes={inner sep=0pt,outer sep=0pt,minimum width=5mm},column sep={5mm,between origins},right=of text]
  {
    49 & 52 & 4C & 49 & 42 & 53 & 59 & 52 \\
  };

  \draw ([xshift=-2.5mm,yshift=0.5em]utf8-1-1.north) rectangle ([xshift=2.5mm,yshift=-0.5em]utf8-1-8.south);

  \foreach \x in {1,...,7}
    \draw ([xshift=2.5mm,yshift=0.5em]utf8-1-\x.north) -- ([xshift=2.5mm,yshift=-0.5em]utf8-1-\x.south);

  \matrix (oce) [matrix of nodes,nodes in empty cells,nodes={inner sep=0pt,outer sep=0pt,minimum width=5mm},column sep={5mm,between origins},right=of utf8]
  {
    C9 & B3 & 85 & C9 & 56 & 0F & A7 & B3 \\
  };

  \draw ([xshift=-2.5mm,yshift=0.5em]oce-1-1.north) rectangle ([xshift=2.5mm,yshift=-0.5em]oce-1-8.south);

  \foreach \x in {1,...,7}
    \draw ([xshift=2.5mm,yshift=0.5em]oce-1-\x.north) -- ([xshift=2.5mm,yshift=-0.5em]oce-1-\x.south);

  \path [every edge/.style={draw,semithick,->}]
    (text) edge node [above] {\footnotesize UTF-8} (utf8)
    (utf8) edge node [above] {\footnotesize OCE} (oce);

\end{tikzpicture}
\caption{The chain \texttt{UTF-8|OCE} applied to text ``IRLIBSYR''\label{fig:concrete-chain}}
\end{figure*}

Character encodings are a special case of something more general. Character encodings map (sequences of) code points to sequences of bytes. One could also imagine transformations which map (sequences of) code points to (sequences of) code points, and sequences of bytes to sequences of bytes, and that such transformations could be chained together, as in Figure~\ref{fig:abs-chain}. (\emph{Composed} would be the mathematical term here.) For example, Caesar ciphers (a.k.a.\ monoalphabetic substitution ciphers) can be thought of as maps from bytes to bytes. It is not far-fetched to think that one might want to search for text which is both encoded and transformed in one of these additional ways.

A prime example of this is Outlook Compressible Encryption, which, despite its name, is just a Caesar cipher applied to mailboxes used by Microsoft's ubiquitous Outlook email client. If you are thinking that this provides no real security because Caesar ciphers are easily defeated by frequency analysis, you would be correct---nonetheless, OCE is an impediment to searching Outlook mailbox files, since it changes the byte patterns contained therein. This forces the investigator either to hand-adjust patterns to account for OCE (which requires knowledge of both the target character encoding and OCE), or to decypher the OCE'd portions of Outlook mailboxes before searching them (which works only if you know you have such files, and probably not on deleted mailbox fragments in a drive's slack space). A better solution---one which we have implemented---generalizes specifying multiple encodings for each pattern by letting the user specify one or more \emph{transformation chains} per pattern. Continuing with the example above, specifying the chain \texttt{UTF-8|OCE} on the pattern \texttt{IRLIBSYR} would cause the byte sequence searched for to be UTF-8-encoded, then OCE'd, as seen in Figure~\ref{fig:concrete-chain}, thus permitting us to search for hits for this pattern in OCE'd Outlook mailboxes directly, without any human expertise required.

\section{Interpreting Hits in Context}
% in which we describe issues with decoding hit context

Search hits can, in isolation from their context, be uninformative, hard to interpret, or misleading. For example, consider that each of ``cabomba'', ``bombastic'', and ``dirty bomb'' contains a hit for \re{bomb}---but the context, the characters on either side of the hit, are essential for recognizing the hit as relevant to aquatic plants, rhetoric, or terrorism. 

It is possible to provide minimal context support by padding regexes with dots, front and back, thus rolling context into the hits. E.g., \re{.\{0,6\}bomb.\{0,6\}} grabs enough on either side of ``bomb'' in the example above to distinguish the three cases, at the expense of a massive, unacceptable slowdown in searching. The reason for poor performance when capturing leading context as part of a hit is straightforward: A match for \re{.\{0,6\}} can start anywhere in the input, so the regex engine has far more work to do when faced with \re{.\{0,6\}bomb.\{0,6\}} than with \re{bomb}.

The problem of context is exacerbated by encodings. One might simply wave off the difficulties with rolling context into the hit by manually examining the data, using \texttt{less} or a hex editor. While this is not an unreasonable thing to do if the data is ASCII, any investigator relying on this strategy will soon come to grief if the data is in a character encoding he is unable to read directly. Even worse, the data might be base64'd, OCE'd, or XOR'd with some constant byte value, which the investigator will then need to decode. Decoding the context manually could involve finding a starting point which available decoders recognize as valid, which costs yet more time and trouble. In such cases, the simple expedient of inspecting the data to see hit context falls down.

In Section~\ref{sec:multi}, we showed how to put multipattern search to use for multiencoding search, and in Section~\ref{sec:chains}, we described a method for chaining encodings together to make multiply-encoded data easily searchable. A consequence of these is that when we return a hit, we know which pattern and encoding chain produced the hit. Because we know the encoding chain which produced the hit, we can reverse that encoding chain to decode the context around the hit and display the decoded context to the user.

Sadly, this is not as straightforward nor are the results as univocal as we've made them sound. There are numerous ways in which decoding context might fail. The simplest is that the context is not in the same encoding as the hit. This can happen when searching serialized binary data, such as database files, where the fields adjancent to the one containing the hit might have different types from the hit. E.g., if an integer field preceeds a hit in a UTF-16LE text field, then interpreting the part of the leading context overlapping the integer field as UTF-16LE is bound to fail to decode, or if it does decode, to produce garbage. Similarly, a hit and its context could be in different files. One way this could occur is when searching a raw disk image. If a hit runs up to the end of a file system block, there is no guarantee that the next byte on the disk belongs to the same file, either because the file containing the hit is fragmented or because the hit is actually at the end of its file. In either case, the trailing context could be anything, so there is no guarantee that it will decode along with the hit. Similar things can occur when searching unallocated space: the bytes preeceding or trailing the hit could be associated with the hit, or could be part of some file which overwrote part of the hit's file and was then deleted itself. Another way that decoding context can fail is if the context was written with a broken encoder. This is not so far-fetched as it sounds. In the early days of Unicode, there were numerous encoders which failed to follow the standard. (A common example of this was writing overlong character representations in UTF-8.) As we cannot anticipate all the possible ways in which encoders might be broken, we cannot properly decode improperly-encoded byte sequences.\footnote{This puts one in mind of Charles Babbage being asked whether by putting the wrong figures into his Difference Engine, the right answers might come out.}

Finally, it could be the case that the hit's context fails to decode because the hit has been misidentified. For example, ASCII is interoperable with many encodings---UTF-7, UTF-8, all sixteen of the ISO-8859-X encodings, Shift JIS, and GB18030, among others. This means that strings consisting entirely of 7-bit ASCII characters are represented by the same byte sequence in many encodings. Consequently, you cannot deduce the encoding from such sequences, and you are likely to get \emph{mojibake} if you have guessed the wrong encoding for the surrounding context . Mojibake (
%文字化け
), which literally means ``character transformation'', is the delightful term borrowed from Japanese for the result of rendering text in the wrong encoding. If, for example, we searched the previous sentence (which we assert, for the purposes of the example, to be UTF-8) for the pattern \re{Mojibake} encoded in Windows-1252 with fifteen characters of trailing context, then our hit plus context would be reported as ``Mojibake (æ–‡å\ —åŒ–ã\ ‘)''. Figure~\ref{fig:mojibake} shows the carnage in detail.\footnote{Note that the bytes AD and 81 are shown as blanks because in Windows-1252, byte AD is \textsc{soft hyphen} (U+AD), which is a non-printing character, and byte 81 is unassigned.}

\begin{figure}[ht]
\centering
\small
\begin{tikzpicture}

  \matrix (m) [matrix of nodes,nodes in empty cells,nodes={inner sep=0pt,outer sep=0pt,minimum width=5mm},column sep={5mm,between origins},row sep={8mm,between origins}]
  {
    ( &
%文
&&&
%字
&&&
%化
&&&
%け
&&& ) \\
    28 & E6 & 96 & 87 & E5 & AD & 97 & E5 & 8C & 96 & E3 & 81 & 91 & 29 \\
    (  & æ  & –  & ‡  & å  &    & —  & å  & Œ  & –  & ã  &    & ‘  & )  \\
  };

  \draw ([xshift=-2.5mm,yshift=0.5em]m-2-1.north) rectangle ([xshift=2.5mm,yshift=-0.5em]m-2-14.south);

  \foreach \x in {1,...,13}
    \draw ([xshift=2.5mm,yshift=0.5em]m-2-\x.north) -- ([xshift=2.5mm,yshift=-0.5em]m-2-\x.south);

  \begin{pgfonlayer}{background}
    \foreach \x in {1,...,14}
      \draw [line width=1mm,color=gray!30,->] (m-1-\x |- m-1-1.south) -- (m-3-\x |- m-3-1.north);

    \draw [line width=1mm,color=gray!30] ([xshift=-0.5mm,yshift=-1mm]m-1-2.south) -- ([xshift=0.5mm,yshift=-1mm]m-1-4.south);
    \draw [line width=1mm,color=gray!30] ([xshift=-0.5mm,yshift=-1mm]m-1-5.south) -- ([xshift=0.5mm,yshift=-1mm]m-1-7.south);
    \draw [line width=1mm,color=gray!30] ([xshift=-0.5mm,yshift=-1mm]m-1-8.south) -- ([xshift=0.5mm,yshift=-1mm]m-1-10.south);
    \draw [line width=1mm,color=gray!30] ([xshift=-0.5mm,yshift=-1mm]m-1-11.south) -- ([xshift=0.5mm,yshift=-1mm]m-1-13.south);
  \end{pgfonlayer}

\end{tikzpicture}
\caption{UTF-8 misinterpred as Windows-1252\label{fig:mojibake}}
\end{figure}

An even stranger thing can happen with UTF-8 and UTF-16LE: There are UTF-16LE strings which contain completely different UTF-8 strings as prefixes. For example the byte sequence which is ``nonsense'' in UTF-8 is ``
%浵捩摥
'' in UTF-16LE (!) and not only that, both can manage to be correctly null-terminated for their encoding (!!). Figure~\ref{fig:8or16} shows how. The explanation for this is that lowercase Latin letters in ASCII fall in the byte range $[61,7\mathrm{A}]$, while all four-digit code points starting with hex digits 6 or 7 are Chinese ideographs. (This applies also to uppercase Latin letters in ASCII, $[41,5\mathrm{A}]$, since four-digit code points starting with 4 or 5 are also Chinese ideographs.)

We do not know whether %潮獮湥敳
is intelligible Chinese, but we suppose that there must be some byte sequences which are both meaningful Chinese when decoded from UTF-16LE and words in some language using the Latin alphabet when decoded from UTF-8.
\begin{figure}[ht]
\centering
\small
\begin{tikzpicture}

  \matrix (m) [matrix of nodes,nodes in empty cells,nodes={inner sep=0pt,outer sep=0pt,minimum width=5mm},column sep={5mm,between origins},row sep={8mm,between origins}]
  {
    n  & o  & n  & s  & e  & n  & s  & e  &    & \\
    6E & 6F & 6E & 73 & 65 & 6E & 73 & 65 & 00 & 00 \\
       &    &    &    &    &    &    &    &    & \\
% 潮獮湥敳 
  };

  \draw ([xshift=-2.5mm,yshift=0.5em]m-2-1.north) rectangle ([xshift=2.5mm,yshift=-0.5em]m-2-10.south);

  \foreach \x in {1,...,9}
    \draw ([xshift=2.5mm,yshift=0.5em]m-2-\x.north) -- ([xshift=2.5mm,yshift=-0.5em]m-2-\x.south);

  \begin{pgfonlayer}{background}
    \foreach \x in {1,...,9}
      \draw [line width=1mm,color=gray!30,<-] (m-1-\x |- m-1-1.south) -- ([yshift=-3mm]m-3-\x |- m-2-1.south);

    \draw [line width=1mm,color=gray!30] ([xshift=-0.5mm,yshift=-3mm]m-2-1.south) -- ([xshift=0.5mm,yshift=-3mm]m-2-2.south);
    \draw [line width=1mm,color=gray!30] ([xshift=-0.5mm,yshift=-3mm]m-2-3.south) -- ([xshift=0.5mm,yshift=-3mm]m-2-4.south);
    \draw [line width=1mm,color=gray!30] ([xshift=-0.5mm,yshift=-3mm]m-2-5.south) -- ([xshift=0.5mm,yshift=-3mm]m-2-6.south);
    \draw [line width=1mm,color=gray!30] ([xshift=-0.5mm,yshift=-3mm]m-2-7.south) -- ([xshift=0.5mm,yshift=-3mm]m-2-8.south);
    \draw [line width=1mm,color=gray!30] ([xshift=-0.5mm,yshift=-3mm]m-2-9.south) -- ([xshift=0.5mm,yshift=-3mm]m-2-10.south);

    \draw [line width=1mm,color=gray!30,->] (m-2-2.south) -- (m-3-2.north);
    \draw [line width=1mm,color=gray!30,->] (m-2-4.south) -- (m-3-4.north);
    \draw [line width=1mm,color=gray!30,->] (m-2-6.south) -- (m-3-6.north);
    \draw [line width=1mm,color=gray!30,->] (m-2-8.south) -- (m-3-8.north);
    \draw [line width=1mm,color=gray!30,->] (m-2-10.south) -- (m-3-10.north);

  \end{pgfonlayer}

\end{tikzpicture}
\caption{Bytes in UTF-8 or UTF-16LE\label{fig:8or16}}
\end{figure}

Finally, a word on what encoding bytes \emph{are}: Bytes can be decoded or not, but don't themselves have an encoding. Bytes are just bytes. All we observe is whether we can decode a byte sequence successfully. As the above discussion makes plain, decoding a sequence without error is no guarantee that the bytes were written with that encoding. The lesson here from philosophy of science is that we can rule out hypotheses from observation, but never prove them conclusively. Hence, we can assert that a given sequence of bytes, say D8 00 DC 00 D8 00 DC 01, \emph{could not} have been written with a (correct) UTF-8 encoder, but only that this sequence is \emph{consistent} with being written by a UTF-16LE encoder.\footnote{In fact, it would decode as the first two glyphs of the yet-undeciphered Linear B language from ancient Crete.} This is not to say that we cannot draw positive conclusions. The longer the sequence which can be validly decoded, the less likely it is to be accidental or mojibake. Encoded text tends to be highly structured when viewed as byte sequences. E.g., 100 random bytes will decode to 7-bit ASCII only one in $2^{100}$ times, because in 7-bit ASCII the high bit in each byte is always off. Similar things may be said about other encodings (though calculating the precise odds for, say, UTF-8, is not so easy.) Thus, \emph{ceteris paribus}, it is probable that long sequences which decode successfully were written in the encoding they appear to have been written in.

There are two implementation details regarding decoding hit context which are of interest: whether to measure the width of the context around a hit in bytes or in characters, and where in the context decoding should start.

Measuring context width in bytes has the advantage that we can trivially locate the start of leading context and the end of trailing context in the data, because these are simply offsets from the start and end of the hit, which we already know. Unfortunately, this is the only advantage of measuring context width in bytes. Measuring in bytes means that the limits of the context could fall in the middle of a character, and that the number of decoded characters for the same number of context bytes can vary from encoding to encoding. If the user wants decoded context, then we already know that the user is thinking in terms of characters, so presumably the number of context characters is more salient for the user than the number of bytes. Furthermore, unless the user is familiar with the technical details of the encoding in use, he will not know how many characters to expect a given number of bytes to produce when decoded. Therefore, we chose to let the user specify the width of the context in characters, not bytes.

The issue of where context decoding should start is more complex. A na\"ive strategy would be to start decoding at the leading end of the context, across the hit, and through to the trailing end. This fails if, for example, the hit is too close to to the start of the data to fill out the context, as in Figure~\ref{fig:alignment}. If we start decoding UTF-16LE from the left end of the buffer, we get the Chinese ideographs shown below, while if we decode from the start of the hit (highlighted) we get a sequence of \textsc{latin small letter x}. Clearly we must ensure that we have proper byte alignment when the decoder reaches the start of the hit. 

\begin{figure}[ht]
\centering
\small
\begin{tikzpicture}

  \matrix (m) [matrix of nodes,nodes in empty cells,nodes={inner sep=0pt,outer sep=0pt,minimum width=5mm},column sep={5mm,between origins},row sep={8mm,between origins}]
  {
    78 & 78 & 00 & 78 & 00 & 78 & 00 & 78 & 00 \\
  };

  \draw ([xshift=-2.5mm,yshift=0.5em]m-1-1.north) rectangle ([xshift=2.5mm,yshift=-0.5em]m-1-9.south);

  \foreach \x in {1,...,8}
    \draw ([xshift=2.5mm,yshift=0.5em]m-1-\x.north) -- ([xshift=2.5mm,yshift=-0.5em]m-1-\x.south);

  \begin{pgfonlayer}{background}
    \draw [line width=10pt,color=gray!30,cap=round] (m-1-2.center) -- (m-1-7.center);
  \end{pgfonlayer}

  \draw [semithick,decorate,decoration={brace,amplitude=5pt,raise=6pt}] (m-1-2.north west) -- (m-1-3.north east) node [midway,above=10pt,text centered] {x};
  \draw [semithick,decorate,decoration={brace,amplitude=5pt,raise=6pt}] (m-1-4.north west) -- (m-1-5.north east) node [midway,above=10pt,text centered] {x};
  \draw [semithick,decorate,decoration={brace,amplitude=5pt,raise=6pt}] (m-1-6.north west) -- (m-1-7.north east) node [midway,above=10pt,text centered] {x};
  \draw [semithick,decorate,decoration={brace,amplitude=5pt,raise=6pt}] (m-1-8.north west) -- (m-1-9.north east) node [midway,above=10pt,text centered] {x};

% 硸砀砀砀

  \draw [semithick,decorate,decoration={brace,amplitude=5pt,raise=6pt}] (m-1-2.south east) -- (m-1-1.south west) node [midway,above=10pt,text centered] {};
  \draw [semithick,decorate,decoration={brace,amplitude=5pt,raise=6pt}] (m-1-4.south east) -- (m-1-3.south west) node [midway,above=10pt,text centered] {};
  \draw [semithick,decorate,decoration={brace,amplitude=5pt,raise=6pt}] (m-1-6.south east) -- (m-1-5.south west) node [midway,above=10pt,text centered] {};
  \draw [semithick,decorate,decoration={brace,amplitude=5pt,raise=6pt}] (m-1-8.south east) -- (m-1-7.south west) node [midway,above=10pt,text centered] {};

\end{tikzpicture}
\caption{Bad alignment when decoding UTF-16LE\label{fig:alignment}}
\end{figure}

A little reflection on the examples above reveals that this is still inadequate, becuase there could be parts of the context which fail to decode. (E.g., suppose the context spans a field boundary in a database file.) We could work around this by advancing by one byte and resetting the decoder each time we encounter an invalid byte, but this too can fail catastrophically.
% TODO: example
Because there is no guarantee that the bytes preceeding the hit were encoded at the same time as the hit, or are even related to it, the only way we can ensure that the hit is decoded successfully according to the encoding chain of the pattern which mathced it is to begin decoding from the start of the it, not from some point in its preceeding context.

Furthermore, the problem of misdecoding the hit by beginning to decode to its left applies equally to decode the leading context. Starting from the far left of the context window has the potential to create mojibake by the time the decoder reaches the end of the leading context. How, then, to decode the preceeding context?

Consider the common case of a hit in the middle of some text. What we would ideally like to do is decode the longest sequences possible which are contiguous with the hit. For the trailing context, this means that decoding may continue rightwards after finishing the hit. For the leading context, it measn that the most important character to decode is the one immediately before the hit, with importance decreasing lefwards. Suppose the hit is the byte sequence $b_m\dotsb b_n$. Then we first try to decode only $b_{m-1}$, the byte to the left of the hit. If that succeeds, we then try to decode $b_{m-2}b_{m-1}$, and continue to extend our decoding window to the left one byte at a time until eithe decoding fails or we reach the limit of the context. If we hit the context limit, we are done---the entirety of the leading context has decoded successfully. If decoding fails, we keep the longest successfully decoded sequence contiguous with the hit.
% FIXME: not a great description of the algorithm



\section{Conclusion}
% in which we say what we have said

\bibliographystyle{elsarticle-num}
\bibliography{enc}

\end{document}
